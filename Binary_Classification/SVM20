import time
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from sklearn.svm import SVC

# ======================================================
# Logging helper
# ======================================================

last_time = time.time()

def log(msg):
    global last_time
    now = time.time()
    elapsed = now - last_time
    print(f"[{time.strftime('%H:%M:%S')}] {msg} | +{elapsed:.2f}s")
    last_time = now

# ======================================================
log("STARTING SCRIPT")

# 1) Load TRAIN and TEST
# ======================================================

log("Loading datasets...")
train_path = "/kaggle/input/start-up-founder-retention-prediction/train.csv"
test_path  = "/kaggle/input/start-up-founder-retention-prediction/test.csv"

df = pd.read_csv(train_path)
df_ext_test = pd.read_csv(test_path)

log(f"Train shape: {df.shape}, Test shape: {df_ext_test.shape}")

# ======================================================
# 2) Column Groups
# ======================================================
log("Setting column groups...")

numeric_cols = [
    "founder_id", "founder_age", "years_with_startup",
    "monthly_revenue_generated", "funding_rounds_led",
    "distance_from_investor_hub", "num_dependents"
]

ordinal_cols = [
    "founder_visibility","startup_reputation","team_size_category",
    "startup_stage","startup_performance_rating","venture_satisfaction",
    "work_life_balance_rating"
]

categorical_cols = [
    "founder_gender","founder_role","education_background",
    "personal_status","innovation_support"
]

boolean_cols = ["working_overtime","remote_operations","leadership_scope"]
target_col = "retention_status"

# ======================================================
# 3) Normalize Boolean Columns
# ======================================================
log("Normalizing boolean columns...")

def normalize_boolean(col):
    return (
        col.astype(str)
           .str.strip()
           .str.lower()
           .map({"true":1,"false":0,"yes":1,"no":0,"1":1,"0":0})
           .astype("Int64")
    )

for col in boolean_cols:
    df[col] = normalize_boolean(df[col])
    df_ext_test[col] = normalize_boolean(df_ext_test[col])

log("Boolean normalization done.")

# ======================================================
# 4) Ordinal Encoding
# ======================================================
log("Applying ordinal encoding...")

ordinal_mappings = {
    "founder_visibility": {"low":0,"medium":1,"high":2,"very high":3},
    "startup_reputation": {"poor":0,"fair":1,"good":2,"excellent":3},
    "team_size_category": {"small":0,"medium":1,"large":2},
    "startup_stage": {"entry":0,"mid":1,"senior":2},
    "startup_performance_rating": {"low":0,"below average":1,"average":2,"high":3},
    "venture_satisfaction": {"low":0,"medium":1,"high":2,"very high":3},
    "work_life_balance_rating": {"fair":0,"good":1,"excellent":2}
}

def apply_ordinal(df, col, mapping):
    df[col] = df[col].astype(str).str.strip().str.lower().map(mapping)

for col in ordinal_cols:
    apply_ordinal(df, col, ordinal_mappings[col])
    apply_ordinal(df_ext_test, col, ordinal_mappings[col])

log("Ordinal encoding done.")

# ======================================================
# 5) Convert Target
# ======================================================
log("Converting target variable...")

df[target_col] = df[target_col].map({"Stayed":1, "Left":0})

# ======================================================
# 6) Train/Validation Split
# ======================================================
log("Splitting train/validation sets...")

X = df.drop(columns=[target_col])
y = df[target_col]

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)

log(f"Train size: {X_train.shape}, Val size: {X_val.shape}")

# ======================================================
# 7) Preprocessing Pipelines
# ======================================================
log("Building preprocessing pipelines...")

skewed_numeric_cols = ["monthly_revenue_generated", "distance_from_investor_hub"]
skewed_numeric_cols = [c for c in skewed_numeric_cols if c in numeric_cols]
skew_indices = [numeric_cols.index(c) for c in skewed_numeric_cols]

def log_transform_selected(X):
    X = X.copy().astype(float)
    for idx in skew_indices:
        X[:, idx] = np.log1p(np.clip(X[:, idx], 0, None))
    return X

numeric_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("log", FunctionTransformer(log_transform_selected, validate=False)),
    ("scale", StandardScaler())
])

def make_ohe():
    if sklearn.__version__ >= "1.2":
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    return OneHotEncoder(handle_unknown="ignore", sparse=False)

categorical_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="most_frequent")),
    ("ohe", make_ohe())
])

boolean_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="most_frequent"))
])

ordinal_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="most_frequent")),
    ("scale", StandardScaler())
])

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, numeric_cols),
    ("ord", ordinal_pipeline, ordinal_cols),
    ("cat", categorical_pipeline, categorical_cols),
    ("bool", boolean_pipeline, boolean_cols)
])

log("Preprocessing setup ready.")

# ======================================================
# 8) Train FIRST SVM Model
# ======================================================
log("Training SVM model... This may take a while.")

svm = SVC(
    kernel="rbf",
    C=1.5,
    gamma="scale",
    probability=True,
    random_state=42
)

clf = Pipeline([
    ("preprocess", preprocessor),
    ("model", svm)
])

clf.fit(X_train, y_train)
log("SVM training completed.")

# ======================================================
# 8B) SECOND SVM TRAINED ON 20% OF TRAIN.CSV
# ======================================================
log("Preparing 20% subset of full train data for second SVM...")

df_small = df.sample(frac=0.20, random_state=42, replace=False)

X_small = df_small.drop(columns=[target_col])
y_small = df_small[target_col]

log(f"Small subset size: {X_small.shape}")

svm_small = SVC(
    kernel="rbf",
    C=1.5,
    gamma="scale",
    probability=True,
    random_state=42
)

clf_small = Pipeline([
    ("preprocess", preprocessor),
    ("model", svm_small)
])

log("Training second SVM on 20% data...")
clf_small.fit(X_small, y_small)
log("Second SVM training completed.")

# ======================================================
# 9B) Stratified K-Fold for 20% SVM
# ======================================================

log("Starting Stratified K-fold for 20% SVM...")

skf2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
small_fold_f1 = []

for fold_idx, (train_idx, val_idx) in enumerate(skf2.split(X_small, y_small), 1):
    X_tr = X_small.iloc[train_idx]
    y_tr = y_small.iloc[train_idx]
    
    X_va = X_small.iloc[val_idx]
    y_va = y_small.iloc[val_idx]

    clf_fold = Pipeline([
        ("preprocess", preprocessor),
        ("model", SVC(
            kernel="rbf",
            C=1.5,
            gamma="scale",
            probability=True,
            random_state=42
        ))
    ])

    clf_fold.fit(X_tr, y_tr)
    y_pred = clf_fold.predict(X_va)

    f1 = f1_score(y_va, y_pred)
    small_fold_f1.append(f1)

    print(f"20% SVM â†’ Fold {fold_idx} F1: {f1:.4f}")

log("20% SVM K-fold finished.")
print("\n20% SVM Average F1 score:", np.mean(small_fold_f1))

# ======================================================
# 9) K-Fold Evaluation (Original Full SVM on Validation Split)
# ======================================================
log("Starting K-fold validation...")

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
fold_f1, fold_auc = [], []
combined_cm = np.zeros((2,2), dtype=int)

X_val_df = X_val.reset_index(drop=True)
y_val_arr = y_val.reset_index(drop=True).to_numpy()

for fold, (_, idx) in enumerate(skf.split(X_val_df, y_val_arr), 1):
    log(f"Evaluating Fold {fold}...")
    
    X_fold = X_val_df.iloc[idx]
    y_fold = y_val_arr[idx]

    y_pred = clf.predict(X_fold)
    y_prob = clf.predict_proba(X_fold)[:, 1]

    f1 = f1_score(y_fold, y_pred)
    auc = roc_auc_score(y_fold, y_prob)
    cm = confusion_matrix(y_fold, y_pred)

    fold_f1.append(f1)
    fold_auc.append(auc)
    combined_cm += cm

    print(f"Fold {fold} - F1={f1:.4f}, AUC={auc:.4f}")
    print(cm)

log("K-fold validation finished.")
print("\nAverage F1:", np.mean(fold_f1))
print("Average AUC:", np.mean(fold_auc))
print("Combined Confusion Matrix:\n", combined_cm)

# ======================================================
# 10) Retrain on Full Data
# ======================================================
log("Training final model on full dataset...")

clf_full = Pipeline([
    ("preprocess", preprocessor),
    ("model", svm)
])

clf_full.fit(X, y)
log("Full training completed.")

# ======================================================
# 11) Predict on External Test
# ======================================================
log("Predicting on external test...")

ext_pred = clf_full.predict(df_ext_test)
ext_pred_label = np.where(ext_pred == 1, "Stayed", "Left")

submission = pd.DataFrame({
    "founder_id": df_ext_test["founder_id"],
    "retention_status": ext_pred_label
})

submission.to_csv("submission.csv", index=False)
log("submission.csv saved!")

print(submission.head())
log("SCRIPT FINISHED.")
