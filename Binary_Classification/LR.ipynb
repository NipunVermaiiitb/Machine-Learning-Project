{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120692,"databundleVersionId":14435901,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:13:23.833445Z","iopub.execute_input":"2025-11-27T12:13:23.833672Z","iopub.status.idle":"2025-11-27T12:13:26.773701Z","shell.execute_reply.started":"2025-11-27T12:13:23.833652Z","shell.execute_reply":"2025-11-27T12:13:26.772500Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/start-up-founder-retention-prediction/sample_submission.csv\n/kaggle/input/start-up-founder-retention-prediction/train.csv\n/kaggle/input/start-up-founder-retention-prediction/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\nfrom xgboost import XGBClassifier\n\n# ======================================================\n# 1) Load TRAIN and EXTERNAL TEST CSVs\n# ======================================================\n\ntrain_path = \"/kaggle/input/start-up-founder-retention-prediction/train.csv\"\ntest_path  = \"/kaggle/input/start-up-founder-retention-prediction/test.csv\"\n\ndf = pd.read_csv(train_path)\ndf_ext_test = pd.read_csv(test_path)\n\n# ======================================================\n# 2) FIXED COLUMN GROUPS\n# ======================================================\n\nnumeric_cols = [\n    \"founder_id\",\n    \"founder_age\",\n    \"years_with_startup\",\n    \"monthly_revenue_generated\",\n    \"funding_rounds_led\",\n    \"distance_from_investor_hub\",\n    \"num_dependents\"\n]\n\ncategorical_cols = [\n    \"founder_gender\",\n    \"founder_role\",\n    \"work_life_balance_rating\",\n    \"venture_satisfaction\",\n    \"startup_performance_rating\",\n    \"education_background\",\n    \"personal_status\",\n    \"startup_stage\",\n    \"team_size_category\",\n    \"years_since_founding\",\n    \"innovation_support\",\n    \"startup_reputation\"\n]\n\nboolean_cols = [\n    \"working_overtime\",\n    \"remote_operations\",\n    \"founder_visibility\",\n    \"leadership_scope\"\n]\n\ntarget_col = \"retention_status\"\n\n# ======================================================\n# 3) Normalize Booleans (handles messy labels)\n# ======================================================\n\ndef normalize_boolean(col):\n    return (\n        col.astype(str)\n           .str.strip()\n           .str.lower()\n           .map({\n               \"true\": 1, \"false\": 0,\n               \"yes\": 1, \"no\": 0,\n               \"1\": 1, \"0\": 0\n           })\n           .astype(\"Int64\")\n    )\n\n# TRAIN\nfor col in boolean_cols:\n    df[col] = normalize_boolean(df[col])\n\n# EXTERNAL TEST\nfor col in boolean_cols:\n    if col in df_ext_test.columns:\n        df_ext_test[col] = normalize_boolean(df_ext_test[col])\n\n# Convert target\ndf[target_col] = df[target_col].map({\"Stayed\": 1, \"Left\": 0})\n\n# ======================================================\n# 4) IDENTIFY AND REMOVE REDUNDANT COLUMNS\n# ======================================================\n\ncols_to_drop = []\n\n# A) Near-zero variance columns\nfor col in numeric_cols + categorical_cols + boolean_cols:\n    if df[col].nunique() <= 1:\n        cols_to_drop.append(col)\n\n# B) High-correlation numeric columns\nnumerics_df = df[numeric_cols].astype(float)\ncorr_matrix = numerics_df.corr().abs()\nupper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\nhigh_corr_cols = [col for col in upper_tri.columns if any(upper_tri[col] > 0.95)]\ncols_to_drop += high_corr_cols\n\n# Remove duplicates in drop list\ncols_to_drop = list(set(cols_to_drop))\n\nprint(\"\\n===== REDUNDANT COLUMNS REMOVED =====\")\nprint(cols_to_drop)\n\n# Remove redundant columns\ndf = df.drop(columns=cols_to_drop)\ndf_ext_test = df_ext_test.drop(columns=cols_to_drop)\n\n# Update column groups\nnumeric_cols = [col for col in numeric_cols if col not in cols_to_drop]\ncategorical_cols = [col for col in categorical_cols if col not in cols_to_drop]\nboolean_cols = [col for col in boolean_cols if col not in cols_to_drop]\n\n# ======================================================\n# 5) 80:20 SPLIT FOR VALIDATION\n# ======================================================\n\nX = df.drop(columns=[target_col])\ny = df[target_col]\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# ======================================================\n# 6) PREPROCESSING PIPELINES\n# ======================================================\n\nskewed_numeric_cols = [\"monthly_revenue_generated\", \"distance_from_investor_hub\"]\nskewed_numeric_cols = [col for col in skewed_numeric_cols if col in numeric_cols]\nskew_indices = [numeric_cols.index(c) for c in skewed_numeric_cols]\n\ndef log_transform_selected(X):\n    X = X.copy().astype(float)\n    for idx in skew_indices:\n        col = X[:, idx]\n        col = np.where(col < 0, 0.0, col)\n        X[:, idx] = np.log1p(col)\n    return X\n\nnumeric_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"log\", FunctionTransformer(log_transform_selected, validate=False)),\n    (\"scaler\", StandardScaler())\n])\n\ndef make_ohe():\n    if sklearn.__version__ >= \"1.2\":\n        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n    else:\n        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\ncategorical_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"ohe\", make_ohe())\n])\n\nboolean_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", numeric_pipeline, numeric_cols),\n    (\"cat\", categorical_pipeline, categorical_cols),\n    (\"bool\", boolean_pipeline, boolean_cols)\n])\nfrom sklearn.linear_model import LogisticRegression\n\n# ======================================================\n# 7) LOGISTIC REGRESSION MODEL (replaces XGBoost)\n# ======================================================\n\nlog_reg = LogisticRegression(\n    penalty=\"l2\",\n    solver=\"lbfgs\",\n    max_iter=500,\n    class_weight=\"balanced\",     # (optional but helps if target is imbalanced)\n    random_state=42\n)\n\nclf = Pipeline(steps=[\n    (\"preprocess\", preprocessor),\n    (\"model\", log_reg)\n])\n\nclf.fit(X_train, y_train)\n\n# ======================================================\n# 8) STRATIFIED K-FOLD VALIDATION\n# ======================================================\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_f1, fold_auc = [], []\ncombined_cm = np.zeros((2, 2), dtype=int)\n\nX_val_df = X_val.reset_index(drop=True)\ny_val_arr = y_val.reset_index(drop=True).to_numpy()\n\nprint(\"\\n===== Validation (20%) with K-Fold =====\")\nfor i, (_, val_idx) in enumerate(skf.split(X_val_df, y_val_arr), 1):\n    X_fold = X_val_df.iloc[val_idx]\n    y_fold = y_val_arr[val_idx]\n\n    y_pred = clf.predict(X_fold)\n    y_prob = clf.predict_proba(X_fold)[:, 1]\n\n    f1 = f1_score(y_fold, y_pred)\n    auc = roc_auc_score(y_fold, y_prob)\n    cm = confusion_matrix(y_fold, y_pred)\n\n    fold_f1.append(f1)\n    fold_auc.append(auc)\n    combined_cm += cm\n\n    print(f\"\\nFold {i} → F1={f1:.4f}, AUC={auc:.4f}\")\n    print(cm)\n\nprint(\"\\n===== FINAL VALIDATION METRICS =====\")\nprint(\"Average F1:\", np.mean(fold_f1))\nprint(\"Average AUC:\", np.mean(fold_auc))\nprint(\"Combined CM:\\n\", combined_cm)\n\n# ======================================================\n# 9) RETRAIN LOGISTIC REGRESSION ON FULL TRAIN.CSV\n# ======================================================\n\nclf_full = Pipeline(steps=[\n    (\"preprocess\", preprocessor),\n    (\"model\", log_reg)\n])\n\nclf_full.fit(X, y)\nprint(\"\\nModel retrained on FULL train.csv ✔\")\n\n# ======================================================\n# 10) PREDICT ON EXTERNAL TEST & EXPORT CSV\n# ======================================================\n\next_pred_binary = clf_full.predict(df_ext_test)\next_pred_label = np.where(ext_pred_binary == 1, \"Stayed\", \"Left\")\n\nsubmission = pd.DataFrame({\n    \"founder_id\": df_ext_test[\"founder_id\"],\n    \"retention_status\": ext_pred_label\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\nsubmission.csv created successfully!\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T13:27:06.434244Z","iopub.execute_input":"2025-11-27T13:27:06.434670Z","iopub.status.idle":"2025-11-27T13:27:20.391882Z","shell.execute_reply.started":"2025-11-27T13:27:06.434622Z","shell.execute_reply":"2025-11-27T13:27:20.390494Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"\n===== REDUNDANT COLUMNS REMOVED =====\n['founder_visibility']\n\n===== Validation (20%) with K-Fold =====\n\nFold 1 → F1=0.7550, AUC=0.8389\n[[873 261]\n [334 917]]\n\nFold 2 → F1=0.7466, AUC=0.8305\n[[854 280]\n [339 912]]\n\nFold 3 → F1=0.7504, AUC=0.8253\n[[838 296]\n [322 929]]\n\nFold 4 → F1=0.7565, AUC=0.8359\n[[847 287]\n [315 935]]\n\nFold 5 → F1=0.7489, AUC=0.8327\n[[827 307]\n [318 932]]\n\n===== FINAL VALIDATION METRICS =====\nAverage F1: 0.7514875479817921\nAverage AUC: 0.8326628502629994\nCombined CM:\n [[4239 1431]\n [1628 4625]]\n\nModel retrained on FULL train.csv ✔\n\nsubmission.csv created successfully!\n   founder_id retention_status\n0       52685           Stayed\n1       30585             Left\n2       54656           Stayed\n3       33442             Left\n4       15667           Stayed\n","output_type":"stream"}],"execution_count":2}]}