{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120692,"databundleVersionId":14435901,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================================================\n# 0. Imports & basic setup\n# ======================================================\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.kernel_approximation import Nystroem\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# ------------------------------------------------------\n# Logging helper\n# ------------------------------------------------------\nlast_time = time.time()\ndef log(msg):\n    global last_time\n    now = time.time()\n    elapsed = now - last_time\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg} | +{elapsed:.2f}s\")\n    last_time = now\n\n# ------------------------------------------------------\n# Reproducibility\n# ------------------------------------------------------\ndef set_seed(seed: int = 42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlog(f\"Using device: {device}\")\n\n# ======================================================\n# 1. Load data\n# ======================================================\nlog(\"Loading datasets...\")\ntrain_path = \"/kaggle/input/start-up-founder-retention-prediction/train.csv\"\ntest_path  = \"/kaggle/input/start-up-founder-retention-prediction/test.csv\"\n\ndf = pd.read_csv(train_path)\ndf_ext_test = pd.read_csv(test_path)\n\nlog(f\"Train shape: {df.shape}, Test shape: {df_ext_test.shape}\")\n\n# ======================================================\n# 2. Column groups & target\n# ======================================================\nlog(\"Setting column groups...\")\n\nnumeric_cols = [\n    \"founder_id\", \"founder_age\", \"years_with_startup\",\n    \"monthly_revenue_generated\", \"funding_rounds_led\",\n    \"distance_from_investor_hub\", \"num_dependents\"\n]\n\nordinal_cols = [\n    \"founder_visibility\", \"startup_reputation\", \"team_size_category\",\n    \"startup_stage\", \"startup_performance_rating\", \"venture_satisfaction\",\n    \"work_life_balance_rating\"\n]\n\ncategorical_cols = [\n    \"founder_gender\", \"founder_role\", \"education_background\",\n    \"personal_status\", \"innovation_support\"\n]\n\nboolean_cols = [\"working_overtime\", \"remote_operations\", \"leadership_scope\"]\ntarget_col = \"retention_status\"\n\n# ======================================================\n# 3. Normalise boolean columns\n# ======================================================\nlog(\"Normalizing boolean columns...\")\n\ndef normalize_boolean(col):\n    return (\n        col.astype(str)\n           .str.strip()\n           .str.lower()\n           .map({\"true\":1,\"false\":0,\"yes\":1,\"no\":0,\"1\":1,\"0\":0})\n           .astype(\"Int64\")\n    )\n\nfor col in boolean_cols:\n    df[col] = normalize_boolean(df[col])\n    df_ext_test[col] = normalize_boolean(df_ext_test[col])\n\nlog(\"Boolean normalization done.\")\n\n# ======================================================\n# 4. Ordinal encoding\n# ======================================================\nlog(\"Applying ordinal encoding...\")\n\nordinal_mappings = {\n    \"founder_visibility\":      {\"low\":0,\"medium\":1,\"high\":2,\"very high\":3},\n    \"startup_reputation\":      {\"poor\":0,\"fair\":1,\"good\":2,\"excellent\":3},\n    \"team_size_category\":      {\"small\":0,\"medium\":1,\"large\":2},\n    \"startup_stage\":           {\"entry\":0,\"mid\":1,\"senior\":2},\n    \"startup_performance_rating\": {\"low\":0,\"below average\":1,\"average\":2,\"high\":3},\n    \"venture_satisfaction\":    {\"low\":0,\"medium\":1,\"high\":2,\"very high\":3},\n    \"work_life_balance_rating\":{\"fair\":0,\"good\":1,\"excellent\":2}\n}\n\ndef apply_ordinal(df_local, col, mapping):\n    df_local[col] = (\n        df_local[col]\n        .astype(str)\n        .str.strip()\n        .str.lower()\n        .map(mapping)\n    )\n\nfor col in ordinal_cols:\n    apply_ordinal(df, col, ordinal_mappings[col])\n    apply_ordinal(df_ext_test, col, ordinal_mappings[col])\n\nlog(\"Ordinal encoding done.\")\n\n# ======================================================\n# 5. Convert target to {0, 1}\n# ======================================================\nlog(\"Converting target variable...\")\ndf[target_col] = df[target_col].map({\"Stayed\":1, \"Left\":0})\n\nX_full = df.drop(columns=[target_col])\ny_full = df[target_col].values\n\n# ======================================================\n# 6. Preprocessing: ColumnTransformer\n# ======================================================\nlog(\"Building preprocessing pipelines...\")\n\n# Highly skewed numeric columns → log1p\nskewed_numeric_cols = [\"monthly_revenue_generated\", \"distance_from_investor_hub\"]\nskewed_numeric_cols = [c for c in skewed_numeric_cols if c in numeric_cols]\nskew_indices = [numeric_cols.index(c) for c in skewed_numeric_cols]\n\ndef log_transform_selected(X):\n    X = X.copy().astype(float)\n    for idx in skew_indices:\n        X[:, idx] = np.log1p(np.clip(X[:, idx], 0, None))\n    return X\n\nnumeric_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"log\", FunctionTransformer(log_transform_selected, validate=False)),\n    (\"scale\", StandardScaler())\n])\n\ndef make_ohe():\n    # Keep compatibility with older sklearn\n    if sklearn.__version__ >= \"1.2\":\n        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n    return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\ncategorical_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"ohe\", make_ohe())\n])\n\nboolean_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n])\n\nordinal_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"scale\", StandardScaler())\n])\n\npreprocessor_template = ColumnTransformer([\n    (\"num\",  numeric_pipeline,   numeric_cols),\n    (\"ord\",  ordinal_pipeline,   ordinal_cols),\n    (\"cat\",  categorical_pipeline, categorical_cols),\n    (\"bool\", boolean_pipeline,   boolean_cols)\n])\n\nlog(\"Preprocessing setup ready.\")\n\n# ======================================================\n# 7. Torch Linear SVM (for Nystroem features)\n# ======================================================\nclass TorchLinearSVM(nn.Module):\n    \"\"\"\n    Simple linear SVM implemented in PyTorch.\n    Uses a single Linear layer and hinge loss.\n    \"\"\"\n    def __init__(self, input_dim):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, 1)  # includes bias term\n\n    def forward(self, x):\n        return self.linear(x).squeeze(1)  # shape: (batch,)\n\ndef hinge_loss(outputs, targets):\n    \"\"\"\n    outputs: raw scores, shape (batch,)\n    targets: in {-1, +1}, shape (batch,)\n    \"\"\"\n    margins = 1.0 - targets * outputs\n    return torch.clamp(margins, min=0).mean()\n\ndef train_torch_svm(\n    X_train_np,\n    y_train_np,\n    epochs=100,\n    batch_size=512,\n    lr=1e-3,\n    weight_decay=1e-2,\n    patience=50,\n    verbose=True\n):\n    \"\"\"\n    Train a linear SVM on top of Nystroem features using PyTorch.\n    Includes early stopping based on validation macro F1.\n    \"\"\"\n\n    # -----------------------------------------------------------\n    # Convert to torch tensors\n    # -----------------------------------------------------------\n    X = torch.tensor(X_train_np, dtype=torch.float32)\n    y_signed = np.where(y_train_np == 1, 1.0, -1.0)\n    y = torch.tensor(y_signed, dtype=torch.float32)\n\n    # -----------------------------------------------------------\n    # Create small validation split (10% of training data)\n    # -----------------------------------------------------------\n    N = len(X)\n    val_size = int(0.10 * N)\n    train_size = N - val_size\n\n    perm = torch.randperm(N)\n    X = X[perm]\n    y = y[perm]\n\n    X_tr, X_val = X[:train_size], X[train_size:]\n    y_tr, y_val = y[:train_size], y[train_size:]\n\n    # Dataloaders\n    train_ds = TensorDataset(X_tr.to(device), y_tr.to(device))\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n\n    # -----------------------------------------------------------\n    # Model + Optimizer\n    # -----------------------------------------------------------\n    model = TorchLinearSVM(input_dim=X_train_np.shape[1]).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    # -----------------------------------------------------------\n    # Early Stopping tracking\n    # -----------------------------------------------------------\n    best_f1 = -1\n    best_state = None\n    no_improve = 0\n\n    # -----------------------------------------------------------\n    # Training loop\n    # -----------------------------------------------------------\n    for epoch in range(1, epochs + 1):\n        model.train()\n        total_loss = 0.0\n\n        for xb, yb in train_loader:\n            optimizer.zero_grad()\n            out = model(xb)\n            loss = hinge_loss(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * xb.size(0)\n\n        avg_loss = total_loss / train_size\n\n        # ---------------------------\n        # Validation evaluation\n        # ---------------------------\n        model.eval()\n        with torch.no_grad():\n            val_scores = model(X_val.to(device)).cpu().numpy()\n            val_pred = (val_scores >= 0).astype(int)\n            val_true = (y_val.cpu().numpy() == 1).astype(int)\n\n            val_f1 = f1_score(val_true, val_pred, average=\"macro\")\n\n        if verbose:\n            print(f\"Epoch {epoch:03d} | Loss={avg_loss:.4f} | Val Macro F1={val_f1:.4f}\")\n\n        # ---------------------------\n        # Early stopping check\n        # ---------------------------\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if no_improve >= patience:\n            if verbose:\n                print(f\"Early stopping triggered at epoch {epoch}.\")\n            break\n\n    # Load best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    if verbose:\n        print(f\"Best Val Macro F1: {best_f1:.4f}\")\n\n    return model\n\n\ndef svm_predict(model, X_np):\n    \"\"\"\n    Predict class labels {0,1} using linear SVM model.\n    \"\"\"\n    model.eval()\n    X_t = torch.tensor(X_np, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        scores = model(X_t).cpu().numpy()\n    # sign threshold at 0: score >= 0 → class 1 (\"Stayed\")\n    y_pred = (scores >= 0.0).astype(int)\n    return y_pred, scores  # scores used for AUC (via sigmoid)\n\n# ======================================================\n# 8. Nystroem + Torch SVM experiment helper\n# ======================================================\ndef run_experiment(\n    experiment_name,\n    X,\n    y,\n    train_fraction=0.8,\n    n_components=512,\n    gamma=\"scale\",\n    epochs=25\n):\n    \"\"\"\n    Runs one experiment:\n      - train_fraction of data used for training\n      - rest used for evaluation\n      - preprocessing + Nystroem + Torch SVM\n      - K-fold on eval split (same as in original SVM script)\n    \"\"\"\n    log(f\"==== {experiment_name}: train_fraction={train_fraction:.2f} ====\")\n\n    X_train, X_eval, y_train, y_eval = train_test_split(\n        X,\n        y,\n        train_size=train_fraction,\n        stratify=y,\n        random_state=42\n    )\n\n    log(f\"{experiment_name}: Train size = {X_train.shape}, Eval size = {X_eval.shape}\")\n\n    # Clone the preprocessor template (fresh instance)\n    import copy\n    preprocessor = copy.deepcopy(preprocessor_template)\n\n    # ------------------------------\n    # Fit preprocessor on train only\n    # ------------------------------\n    log(f\"{experiment_name}: Fitting preprocessor on train...\")\n    preprocessor.fit(X_train)\n    X_train_proc = preprocessor.transform(X_train)\n    X_eval_proc  = preprocessor.transform(X_eval)\n\n    # Ensure dense\n    X_train_proc = np.array(X_train_proc)\n    X_eval_proc  = np.array(X_eval_proc)\n    log(f\"{experiment_name}: Preprocessed shapes: train={X_train_proc.shape}, eval={X_eval_proc.shape}\")\n\n    # ------------------------------\n    # Nystroem kernel approximation\n    # ------------------------------\n    log(f\"{experiment_name}: Computing gamma='scale' numeric value...\")\n\n    # Compute gamma similar to SVC(gamma='scale')\n    var = X_train_proc.var()\n    n_features = X_train_proc.shape[1]\n    gamma_value = 1.0 / (n_features * var)\n    \n    log(f\"{experiment_name}: Using gamma={gamma_value:.6f}\")\n    \n    log(f\"{experiment_name}: Fitting Nystroem (approximate RBF kernel)...\")\n    nystroem = Nystroem(\n        kernel=\"rbf\",\n        gamma=float(gamma_value),\n        n_components=n_components,\n        random_state=42\n    )\n    X_train_nys = nystroem.fit_transform(X_train_proc)\n    X_eval_nys  = nystroem.transform(X_eval_proc)\n\n\n    log(f\"{experiment_name}: Nystroem shapes: train={X_train_nys.shape}, eval={X_eval_nys.shape}\")\n\n    # ------------------------------\n    # Train Torch Linear SVM on Nystroem features\n    # ------------------------------\n    log(f\"{experiment_name}: Training Torch Linear SVM on GPU (if available)...\")\n    model = train_torch_svm(\n        X_train_np=X_train_nys,\n        y_train_np=y_train,\n        epochs=epochs,\n        batch_size=512,\n        lr=1e-3,\n        weight_decay=1e-2,\n        patience=50,\n        verbose=True\n    )\n\n    # ------------------------------\n    # Evaluation on eval split\n    # ------------------------------\n    log(f\"{experiment_name}: Evaluating on eval split...\")\n    y_pred_eval, scores_eval = svm_predict(model, X_eval_nys)\n    # Use sigmoid(scores) as pseudo-probabilities for class 1\n    probs_eval = 1.0 / (1.0 + np.exp(-scores_eval))\n\n    f1 = f1_score(y_eval, y_pred_eval, average=\"macro\")\n    auc = roc_auc_score(y_eval, probs_eval)\n    cm = confusion_matrix(y_eval, y_pred_eval)\n\n    print(f\"\\n[{experiment_name}] Eval F1 = {f1:.4f}, Eval AUC = {auc:.4f}\")\n    print(f\"[{experiment_name}] Confusion matrix:\\n{cm}\")\n\n    # ------------------------------\n    # K-fold on eval split (like original script)\n    # ------------------------------\n    log(f\"{experiment_name}: Starting K-fold evaluation on eval split...\")\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    fold_f1, fold_auc = [], []\n    combined_cm = np.zeros((2, 2), dtype=int)\n\n    X_eval_df = X_eval.reset_index(drop=True)\n    y_eval_arr = pd.Series(y_eval).reset_index(drop=True).to_numpy()\n\n    # For each fold, we *re-use* the same trained model and precomputed Nystroem representation\n    # just like the original SVM script reused the same classifier.\n    for fold, (_, idx) in enumerate(skf.split(X_eval_df, y_eval_arr), 1):\n        log(f\"{experiment_name}: K-Fold {fold}...\")\n\n        X_fold = X_eval_nys[idx]\n        y_fold = y_eval_arr[idx]\n\n        y_pred_fold, scores_fold = svm_predict(model, X_fold)\n        probs_fold = 1.0 / (1.0 + np.exp(-scores_fold))\n\n        f1_fold = f1_score(y_fold, y_pred_fold, average=\"macro\")\n        auc_fold = roc_auc_score(y_fold, probs_fold)\n        cm_fold = confusion_matrix(y_fold, y_pred_fold)\n\n        fold_f1.append(f1_fold)\n        fold_auc.append(auc_fold)\n        combined_cm += cm_fold\n\n        print(f\"  Fold {fold}: F1={f1_fold:.4f}, AUC={auc_fold:.4f}\")\n        print(f\"  Confusion matrix:\\n{cm_fold}\")\n\n    avg_f1 = np.mean(fold_f1)\n    avg_auc = np.mean(fold_auc)\n\n    print(f\"\\n[{experiment_name}] K-fold Avg F1 = {avg_f1:.4f}, Avg AUC = {avg_auc:.4f}\")\n    print(f\"[{experiment_name}] K-fold combined confusion matrix:\\n{combined_cm}\")\n\n    results = {\n        \"experiment\": experiment_name,\n        \"train_fraction\": train_fraction,\n        \"eval_F1\": f1,\n        \"eval_AUC\": auc,\n        \"kfold_F1\": avg_f1,\n        \"kfold_AUC\": avg_auc,\n        \"eval_confusion_matrix\": cm,\n        \"kfold_confusion_matrix\": combined_cm,\n        \"preprocessor\": preprocessor,\n        \"nystroem\": nystroem,\n        \"model\": model\n    }\n    return results\n\n# ======================================================\n# 9. Run experiments:\n#    A) 80/20 on FULL dataset\n#    B) 80/20 on MINI dataset (20% of full)\n# ======================================================\n\nlog(\"===== Creating MINI-DATASET (20% of full dataset) =====\")\n\n# Take 20% of full dataset to simulate \"small dataset\"\nX_mini, _, y_mini, _ = train_test_split(\n    X_full,\n    y_full,\n    train_size=0.20,     # mini dataset = 20% of full\n    stratify=y_full,\n    random_state=42\n)\n\nlog(f\"Mini dataset shape: {X_mini.shape}, y_mini shape: {y_mini.shape}\")\n\n# ======================================================\n# A) Full dataset 80/20 experiment\n# ======================================================\nresults_full_80_20 = run_experiment(\n    experiment_name=\"Full_80_20\",\n    X=X_full,\n    y=y_full,\n    train_fraction=0.80,   # always 80/20\n    n_components=512,\n    gamma=\"scale\"\n)\n\n# ======================================================\n# B) Mini dataset 80/20 experiment\n# ======================================================\nresults_mini_80_20 = run_experiment(\n    experiment_name=\"Mini_80_20\",\n    X=X_mini,\n    y=y_mini,\n    train_fraction=0.80,   # 80/20 split on mini dataset\n    n_components=512,\n    gamma=\"scale\",\n    epochs=100\n)\n\nlog(\"===== Both experiments completed =====\")\n\nprint(\"\\n================= COMPARISON =================\")\nprint(f\"Full_80_20: Train size=80% of 59611 → ~47,000 samples\")\nprint(f\"  Eval F1 = {results_full_80_20['eval_F1']:.4f}  |  KFold F1 = {results_full_80_20['kfold_F1']:.4f}\")\n\nprint(f\"\\nMini_80_20: Train size=80% of 20% mini dataset (~9,500 samples → train ≈ 7,600)\")\nprint(f\"  Eval F1 = {results_mini_80_20['eval_F1']:.4f}  |  KFold F1 = {results_mini_80_20['kfold_F1']:.4f}\")\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:57:35.462114Z","iopub.execute_input":"2025-12-02T16:57:35.462630Z","iopub.status.idle":"2025-12-02T16:57:56.351509Z","shell.execute_reply.started":"2025-12-02T16:57:35.462606Z","shell.execute_reply":"2025-12-02T16:57:56.350735Z"}},"outputs":[{"name":"stdout","text":"[16:57:35] Using device: cuda | +0.00s\n[16:57:35] Loading datasets... | +0.00s\n[16:57:35] Train shape: (59611, 24), Test shape: (14900, 23) | +0.18s\n[16:57:35] Setting column groups... | +0.00s\n[16:57:35] Normalizing boolean columns... | +0.00s\n[16:57:35] Boolean normalization done. | +0.07s\n[16:57:35] Applying ordinal encoding... | +0.00s\n[16:57:35] Ordinal encoding done. | +0.18s\n[16:57:35] Converting target variable... | +0.00s\n[16:57:35] Building preprocessing pipelines... | +0.01s\n[16:57:35] Preprocessing setup ready. | +0.00s\n[16:57:35] ===== Creating MINI-DATASET (20% of full dataset) ===== | +0.00s\n[16:57:35] Mini dataset shape: (11922, 23), y_mini shape: (11922,) | +0.03s\n[16:57:35] ==== Full_80_20: train_fraction=0.80 ==== | +0.00s\n[16:57:36] Full_80_20: Train size = (47688, 23), Eval size = (11923, 23) | +0.03s\n[16:57:36] Full_80_20: Fitting preprocessor on train... | +0.00s\n[16:57:36] Full_80_20: Preprocessed shapes: train=(47688, 34), eval=(11923, 34) | +0.30s\n[16:57:36] Full_80_20: Computing gamma='scale' numeric value... | +0.00s\n[16:57:36] Full_80_20: Using gamma=0.053611 | +0.00s\n[16:57:36] Full_80_20: Fitting Nystroem (approximate RBF kernel)... | +0.00s\n[16:57:36] Full_80_20: Nystroem shapes: train=(47688, 512), eval=(11923, 512) | +0.68s\n[16:57:36] Full_80_20: Training Torch Linear SVM on GPU (if available)... | +0.00s\nEpoch 001 | Loss=0.9594 | Val Macro F1=0.5377\nEpoch 002 | Loss=0.8781 | Val Macro F1=0.5349\nEpoch 003 | Loss=0.8086 | Val Macro F1=0.6274\nEpoch 004 | Loss=0.7565 | Val Macro F1=0.6928\nEpoch 005 | Loss=0.7175 | Val Macro F1=0.6998\nEpoch 006 | Loss=0.6938 | Val Macro F1=0.7051\nEpoch 007 | Loss=0.6781 | Val Macro F1=0.7090\nEpoch 008 | Loss=0.6667 | Val Macro F1=0.7131\nEpoch 009 | Loss=0.6576 | Val Macro F1=0.7140\nEpoch 010 | Loss=0.6502 | Val Macro F1=0.7182\nEpoch 011 | Loss=0.6437 | Val Macro F1=0.7202\nEpoch 012 | Loss=0.6382 | Val Macro F1=0.7220\nEpoch 013 | Loss=0.6334 | Val Macro F1=0.7243\nEpoch 014 | Loss=0.6292 | Val Macro F1=0.7261\nEpoch 015 | Loss=0.6254 | Val Macro F1=0.7289\nEpoch 016 | Loss=0.6221 | Val Macro F1=0.7283\nEpoch 017 | Loss=0.6191 | Val Macro F1=0.7318\nEpoch 018 | Loss=0.6164 | Val Macro F1=0.7318\nEpoch 019 | Loss=0.6139 | Val Macro F1=0.7338\nEpoch 020 | Loss=0.6117 | Val Macro F1=0.7361\nEpoch 021 | Loss=0.6097 | Val Macro F1=0.7356\nEpoch 022 | Loss=0.6077 | Val Macro F1=0.7379\nEpoch 023 | Loss=0.6059 | Val Macro F1=0.7390\nEpoch 024 | Loss=0.6043 | Val Macro F1=0.7406\nEpoch 025 | Loss=0.6028 | Val Macro F1=0.7406\nBest Val Macro F1: 0.7406\n[16:57:47] Full_80_20: Evaluating on eval split... | +10.34s\n\n[Full_80_20] Eval F1 = 0.7354, Eval AUC = 0.8172\n[Full_80_20] Confusion matrix:\n[[3964 1706]\n [1433 4820]]\n[16:57:47] Full_80_20: Starting K-fold evaluation on eval split... | +0.02s\n[16:57:47] Full_80_20: K-Fold 1... | +0.00s\n  Fold 1: F1=0.7444, AUC=0.8237\n  Confusion matrix:\n[[812 322]\n [285 966]]\n[16:57:47] Full_80_20: K-Fold 2... | +0.01s\n  Fold 2: F1=0.7386, AUC=0.8171\n  Confusion matrix:\n[[795 339]\n [281 970]]\n[16:57:47] Full_80_20: K-Fold 3... | +0.01s\n  Fold 3: F1=0.7239, AUC=0.8096\n  Confusion matrix:\n[[777 357]\n [298 953]]\n[16:57:47] Full_80_20: K-Fold 4... | +0.01s\n  Fold 4: F1=0.7361, AUC=0.8189\n  Confusion matrix:\n[[795 339]\n [287 963]]\n[16:57:47] Full_80_20: K-Fold 5... | +0.01s\n  Fold 5: F1=0.7337, AUC=0.8169\n  Confusion matrix:\n[[785 349]\n [282 968]]\n\n[Full_80_20] K-fold Avg F1 = 0.7354, Avg AUC = 0.8172\n[Full_80_20] K-fold combined confusion matrix:\n[[3964 1706]\n [1433 4820]]\n[16:57:47] ==== Mini_80_20: train_fraction=0.80 ==== | +0.01s\n[16:57:47] Mini_80_20: Train size = (9537, 23), Eval size = (2385, 23) | +0.01s\n[16:57:47] Mini_80_20: Fitting preprocessor on train... | +0.00s\n[16:57:47] Mini_80_20: Preprocessed shapes: train=(9537, 34), eval=(2385, 34) | +0.08s\n[16:57:47] Mini_80_20: Computing gamma='scale' numeric value... | +0.00s\n[16:57:47] Mini_80_20: Using gamma=0.053621 | +0.00s\n[16:57:47] Mini_80_20: Fitting Nystroem (approximate RBF kernel)... | +0.00s\n[16:57:47] Mini_80_20: Nystroem shapes: train=(9537, 512), eval=(2385, 512) | +0.28s\n[16:57:47] Mini_80_20: Training Torch Linear SVM on GPU (if available)... | +0.00s\nEpoch 001 | Loss=0.9939 | Val Macro F1=0.6783\nEpoch 002 | Loss=0.9765 | Val Macro F1=0.6207\nEpoch 003 | Loss=0.9590 | Val Macro F1=0.5943\nEpoch 004 | Loss=0.9420 | Val Macro F1=0.5600\nEpoch 005 | Loss=0.9245 | Val Macro F1=0.5435\nEpoch 006 | Loss=0.9071 | Val Macro F1=0.5378\nEpoch 007 | Loss=0.8899 | Val Macro F1=0.5354\nEpoch 008 | Loss=0.8725 | Val Macro F1=0.5312\nEpoch 009 | Loss=0.8556 | Val Macro F1=0.5209\nEpoch 010 | Loss=0.8386 | Val Macro F1=0.5319\nEpoch 011 | Loss=0.8237 | Val Macro F1=0.5370\nEpoch 012 | Loss=0.8106 | Val Macro F1=0.5640\nEpoch 013 | Loss=0.7985 | Val Macro F1=0.5832\nEpoch 014 | Loss=0.7871 | Val Macro F1=0.5981\nEpoch 015 | Loss=0.7760 | Val Macro F1=0.6345\nEpoch 016 | Loss=0.7650 | Val Macro F1=0.6511\nEpoch 017 | Loss=0.7545 | Val Macro F1=0.6766\nEpoch 018 | Loss=0.7444 | Val Macro F1=0.6859\nEpoch 019 | Loss=0.7353 | Val Macro F1=0.6904\nEpoch 020 | Loss=0.7270 | Val Macro F1=0.6958\nEpoch 021 | Loss=0.7196 | Val Macro F1=0.6990\nEpoch 022 | Loss=0.7128 | Val Macro F1=0.7039\nEpoch 023 | Loss=0.7069 | Val Macro F1=0.7064\nEpoch 024 | Loss=0.7015 | Val Macro F1=0.7054\nEpoch 025 | Loss=0.6966 | Val Macro F1=0.7077\nEpoch 026 | Loss=0.6922 | Val Macro F1=0.7039\nEpoch 027 | Loss=0.6880 | Val Macro F1=0.7070\nEpoch 028 | Loss=0.6843 | Val Macro F1=0.7072\nEpoch 029 | Loss=0.6807 | Val Macro F1=0.7062\nEpoch 030 | Loss=0.6774 | Val Macro F1=0.7093\nEpoch 031 | Loss=0.6743 | Val Macro F1=0.7102\nEpoch 032 | Loss=0.6715 | Val Macro F1=0.7103\nEpoch 033 | Loss=0.6688 | Val Macro F1=0.7125\nEpoch 034 | Loss=0.6663 | Val Macro F1=0.7128\nEpoch 035 | Loss=0.6638 | Val Macro F1=0.7138\nEpoch 036 | Loss=0.6614 | Val Macro F1=0.7138\nEpoch 037 | Loss=0.6592 | Val Macro F1=0.7148\nEpoch 038 | Loss=0.6571 | Val Macro F1=0.7148\nEpoch 039 | Loss=0.6550 | Val Macro F1=0.7148\nEpoch 040 | Loss=0.6531 | Val Macro F1=0.7138\nEpoch 041 | Loss=0.6512 | Val Macro F1=0.7162\nEpoch 042 | Loss=0.6494 | Val Macro F1=0.7182\nEpoch 043 | Loss=0.6476 | Val Macro F1=0.7183\nEpoch 044 | Loss=0.6459 | Val Macro F1=0.7213\nEpoch 045 | Loss=0.6443 | Val Macro F1=0.7224\nEpoch 046 | Loss=0.6427 | Val Macro F1=0.7224\nEpoch 047 | Loss=0.6410 | Val Macro F1=0.7193\nEpoch 048 | Loss=0.6395 | Val Macro F1=0.7214\nEpoch 049 | Loss=0.6380 | Val Macro F1=0.7203\nEpoch 050 | Loss=0.6365 | Val Macro F1=0.7203\nEpoch 051 | Loss=0.6351 | Val Macro F1=0.7225\nEpoch 052 | Loss=0.6338 | Val Macro F1=0.7214\nEpoch 053 | Loss=0.6325 | Val Macro F1=0.7204\nEpoch 054 | Loss=0.6312 | Val Macro F1=0.7214\nEpoch 055 | Loss=0.6300 | Val Macro F1=0.7214\nEpoch 056 | Loss=0.6287 | Val Macro F1=0.7226\nEpoch 057 | Loss=0.6275 | Val Macro F1=0.7204\nEpoch 058 | Loss=0.6263 | Val Macro F1=0.7204\nEpoch 059 | Loss=0.6251 | Val Macro F1=0.7204\nEpoch 060 | Loss=0.6240 | Val Macro F1=0.7204\nEpoch 061 | Loss=0.6229 | Val Macro F1=0.7183\nEpoch 062 | Loss=0.6218 | Val Macro F1=0.7193\nEpoch 063 | Loss=0.6208 | Val Macro F1=0.7182\nEpoch 064 | Loss=0.6197 | Val Macro F1=0.7182\nEpoch 065 | Loss=0.6187 | Val Macro F1=0.7192\nEpoch 066 | Loss=0.6177 | Val Macro F1=0.7192\nEpoch 067 | Loss=0.6167 | Val Macro F1=0.7192\nEpoch 068 | Loss=0.6158 | Val Macro F1=0.7203\nEpoch 069 | Loss=0.6149 | Val Macro F1=0.7214\nEpoch 070 | Loss=0.6140 | Val Macro F1=0.7203\nEpoch 071 | Loss=0.6131 | Val Macro F1=0.7214\nEpoch 072 | Loss=0.6123 | Val Macro F1=0.7203\nEpoch 073 | Loss=0.6114 | Val Macro F1=0.7203\nEpoch 074 | Loss=0.6106 | Val Macro F1=0.7213\nEpoch 075 | Loss=0.6098 | Val Macro F1=0.7203\nEpoch 076 | Loss=0.6090 | Val Macro F1=0.7224\nEpoch 077 | Loss=0.6082 | Val Macro F1=0.7224\nEpoch 078 | Loss=0.6074 | Val Macro F1=0.7203\nEpoch 079 | Loss=0.6067 | Val Macro F1=0.7203\nEpoch 080 | Loss=0.6060 | Val Macro F1=0.7193\nEpoch 081 | Loss=0.6052 | Val Macro F1=0.7203\nEpoch 082 | Loss=0.6046 | Val Macro F1=0.7213\nEpoch 083 | Loss=0.6039 | Val Macro F1=0.7203\nEpoch 084 | Loss=0.6031 | Val Macro F1=0.7203\nEpoch 085 | Loss=0.6025 | Val Macro F1=0.7203\nEpoch 086 | Loss=0.6018 | Val Macro F1=0.7193\nEpoch 087 | Loss=0.6011 | Val Macro F1=0.7193\nEpoch 088 | Loss=0.6005 | Val Macro F1=0.7183\nEpoch 089 | Loss=0.5999 | Val Macro F1=0.7172\nEpoch 090 | Loss=0.5992 | Val Macro F1=0.7172\nEpoch 091 | Loss=0.5986 | Val Macro F1=0.7182\nEpoch 092 | Loss=0.5980 | Val Macro F1=0.7192\nEpoch 093 | Loss=0.5975 | Val Macro F1=0.7193\nEpoch 094 | Loss=0.5969 | Val Macro F1=0.7171\nEpoch 095 | Loss=0.5963 | Val Macro F1=0.7182\nEpoch 096 | Loss=0.5957 | Val Macro F1=0.7192\nEpoch 097 | Loss=0.5952 | Val Macro F1=0.7202\nEpoch 098 | Loss=0.5946 | Val Macro F1=0.7202\nEpoch 099 | Loss=0.5941 | Val Macro F1=0.7202\nEpoch 100 | Loss=0.5936 | Val Macro F1=0.7191\nBest Val Macro F1: 0.7226\n[16:57:56] Mini_80_20: Evaluating on eval split... | +8.56s\n\n[Mini_80_20] Eval F1 = 0.7316, Eval AUC = 0.8227\n[Mini_80_20] Confusion matrix:\n[[779 355]\n [281 970]]\n[16:57:56] Mini_80_20: Starting K-fold evaluation on eval split... | +0.01s\n[16:57:56] Mini_80_20: K-Fold 1... | +0.00s\n  Fold 1: F1=0.7343, AUC=0.8329\n  Confusion matrix:\n[[148  79]\n [ 46 204]]\n[16:57:56] Mini_80_20: K-Fold 2... | +0.00s\n  Fold 2: F1=0.7250, AUC=0.8287\n  Confusion matrix:\n[[151  76]\n [ 54 196]]\n[16:57:56] Mini_80_20: K-Fold 3... | +0.00s\n  Fold 3: F1=0.7124, AUC=0.8008\n  Confusion matrix:\n[[161  66]\n [ 71 179]]\n[16:57:56] Mini_80_20: K-Fold 4... | +0.00s\n  Fold 4: F1=0.7561, AUC=0.8434\n  Confusion matrix:\n[[168  59]\n [ 57 193]]\n[16:57:56] Mini_80_20: K-Fold 5... | +0.00s\n  Fold 5: F1=0.7290, AUC=0.8100\n  Confusion matrix:\n[[151  75]\n [ 53 198]]\n\n[Mini_80_20] K-fold Avg F1 = 0.7314, Avg AUC = 0.8232\n[Mini_80_20] K-fold combined confusion matrix:\n[[779 355]\n [281 970]]\n[16:57:56] ===== Both experiments completed ===== | +0.01s\n\n================= COMPARISON =================\nFull_80_20: Train size=80% of 59611 → ~47,000 samples\n  Eval F1 = 0.7354  |  KFold F1 = 0.7354\n\nMini_80_20: Train size=80% of 20% mini dataset (~9,500 samples → train ≈ 7,600)\n  Eval F1 = 0.7316  |  KFold F1 = 0.7314\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ======================================================\n# 0. Imports & basic setup\n# ======================================================\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.kernel_approximation import Nystroem\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n\n\n# ------------------------------------------------------\n# Logging helper\n# ------------------------------------------------------\nlast_time = time.time()\ndef log(msg):\n    global last_time\n    now = time.time()\n    elapsed = now - last_time\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg} | +{elapsed:.2f}s\")\n    last_time = now\n\n\n# ------------------------------------------------------\n# Reproducibility\n# ------------------------------------------------------\ndef set_seed(seed: int = 42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlog(f\"Using device: {device}\")\n\n\n# ======================================================\n# 1. Load data\n# ======================================================\nlog(\"Loading datasets...\")\ntrain_path = \"/kaggle/input/start-up-founder-retention-prediction/train.csv\"\ntest_path  = \"/kaggle/input/start-up-founder-retention-prediction/test.csv\"\n\ndf = pd.read_csv(train_path)\ndf_ext_test = pd.read_csv(test_path)\nlog(f\"Train shape: {df.shape}, Test shape: {df_ext_test.shape}\")\n\n\n# ======================================================\n# 2. Column groups & target\n# ======================================================\nnumeric_cols = [\n    \"founder_id\", \"founder_age\", \"years_with_startup\",\n    \"monthly_revenue_generated\", \"funding_rounds_led\",\n    \"distance_from_investor_hub\", \"num_dependents\"\n]\n\nordinal_cols = [\n    \"founder_visibility\", \"startup_reputation\", \"team_size_category\",\n    \"startup_stage\", \"startup_performance_rating\", \"venture_satisfaction\",\n    \"work_life_balance_rating\"\n]\n\ncategorical_cols = [\n    \"founder_gender\", \"founder_role\", \"education_background\",\n    \"personal_status\", \"innovation_support\"\n]\n\nboolean_cols = [\"working_overtime\", \"remote_operations\", \"leadership_scope\"]\ntarget_col = \"retention_status\"\n\n\n\n# ======================================================\n# 3. Normalise boolean columns\n# ======================================================\ndef normalize_boolean(col):\n    return (\n        col.astype(str)\n           .str.strip()\n           .str.lower()\n           .map({\"true\":1,\"false\":0,\"yes\":1,\"no\":0,\"1\":1,\"0\":0})\n           .astype(\"Int64\")\n    )\n\nfor col in boolean_cols:\n    df[col] = normalize_boolean(df[col])\n    df_ext_test[col] = normalize_boolean(df_ext_test[col])\n\n\n\n# ======================================================\n# 4. Ordinal encoding\n# ======================================================\nordinal_mappings = {\n    \"founder_visibility\":      {\"low\":0,\"medium\":1,\"high\":2,\"very high\":3},\n    \"startup_reputation\":      {\"poor\":0,\"fair\":1,\"good\":2,\"excellent\":3},\n    \"team_size_category\":      {\"small\":0,\"medium\":1,\"large\":2},\n    \"startup_stage\":           {\"entry\":0,\"mid\":1,\"senior\":2},\n    \"startup_performance_rating\": {\"low\":0,\"below average\":1,\"average\":2,\"high\":3},\n    \"venture_satisfaction\":    {\"low\":0,\"medium\":1,\"high\":2,\"very high\":3},\n    \"work_life_balance_rating\":{\"fair\":0,\"good\":1,\"excellent\":2}\n}\n\ndef apply_ordinal(df_local, col, mapping):\n    df_local[col] = (\n        df_local[col]\n        .astype(str)\n        .str.strip()\n        .str.lower()\n        .map(mapping)\n    )\n\nfor col in ordinal_cols:\n    apply_ordinal(df, col, ordinal_mappings[col])\n    apply_ordinal(df_ext_test, col, ordinal_mappings[col])\n\n\n\n# ======================================================\n# 5. Convert target to {0,1}\n# ======================================================\ndf[target_col] = df[target_col].map({\"Stayed\":1, \"Left\":0})\n\nX_full = df.drop(columns=[target_col])\ny_full = df[target_col].values\n\n\n\n# ======================================================\n# 6. Preprocessing: ColumnTransformer\n# ======================================================\nskewed_numeric_cols = [\"monthly_revenue_generated\", \"distance_from_investor_hub\"]\nskewed_numeric_cols = [c for c in skewed_numeric_cols if c in numeric_cols]\nskew_indices = [numeric_cols.index(c) for c in skewed_numeric_cols]\n\ndef log_transform_selected(X):\n    X = X.copy().astype(float)\n    for idx in skew_indices:\n        X[:, idx] = np.log1p(np.clip(X[:, idx], 0, None))\n    return X\n\nnumeric_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"log\", FunctionTransformer(log_transform_selected, validate=False)),\n    (\"scale\", StandardScaler())\n])\n\ndef make_ohe():\n    if sklearn.__version__ >= \"1.2\":\n        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n    return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\ncategorical_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"ohe\", make_ohe())\n])\n\nboolean_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n])\n\nordinal_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"scale\", StandardScaler())\n])\n\npreprocessor_template = ColumnTransformer([\n    (\"num\", numeric_pipeline, numeric_cols),\n    (\"ord\", ordinal_pipeline, ordinal_cols),\n    (\"cat\", categorical_pipeline, categorical_cols),\n    (\"bool\", boolean_pipeline, boolean_cols)\n])\n\nlog(\"Preprocessing setup ready.\")\n\n\n\n# ======================================================\n# 7. Neural Network Equivalent to SVM (MLP classifier)\n# ======================================================\nclass NystroemMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, dropout=0.2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.BatchNorm1d(hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(hidden_dim // 2, 2)   # 2 classes: Left / Stayed\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass HungryNystroemMLP(nn.Module):\n    def __init__(self, input_dim, dropout=0.8):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.GELU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(128, 2)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# ======================================================\n# 8. Train NN (early stopping on val macro F1)\n# ======================================================\ndef train_nystroem_nn(\n    X_train_np,\n    y_train_np,\n    epochs=100,\n    batch_size=512,\n    lr=1e-3,\n    weight_decay=1e-1,\n    patience=50,\n    verbose=True\n):\n\n    # Convert\n    X = torch.tensor(X_train_np, dtype=torch.float32)\n    y = torch.tensor(y_train_np, dtype=torch.long)\n\n    # Validation split (10%)\n    N = len(X)\n    val_size = int(0.10 * N)\n    train_size = N - val_size\n\n    perm = torch.randperm(N)\n    X = X[perm]\n    y = y[perm]\n\n    X_tr, X_val = X[:train_size], X[train_size:]\n    y_tr, y_val = y[:train_size], y[train_size:]\n\n    train_ds = TensorDataset(X_tr.to(device), y_tr.to(device))\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n\n    input_dim = X_train_np.shape[1]\n    model = HungryNystroemMLP(input_dim=input_dim).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    criterion = nn.CrossEntropyLoss()\n\n    best_state = None\n    best_f1 = -1\n    no_improve = 0\n\n    # Train loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n\n        for xb, yb in train_loader:\n            optimizer.zero_grad()\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * xb.size(0)\n\n        train_loss = running_loss / train_size\n\n        # Validation macro F1\n        model.eval()\n        with torch.no_grad():\n            logits_val = model(X_val.to(device)).cpu()\n            preds_val = logits_val.argmax(dim=1).numpy()\n            true_val = y_val.numpy()\n\n            val_f1 = f1_score(true_val, preds_val, average=\"macro\")\n\n        if verbose:\n            print(f\"Epoch {epoch:03d} | Loss={train_loss:.4f} | Val Macro F1={val_f1:.4f}\")\n\n        # Early stopping\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_state = model.state_dict()\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if no_improve >= patience:\n            print(f\"Early stopping at epoch {epoch}.\")\n            break\n\n    # Restore best model\n    model.load_state_dict(best_state)\n    print(f\"Best validation macro F1: {best_f1:.4f}\")\n    return model\n\n\n\ndef nn_predict(model, X_np):\n    X_t = torch.tensor(X_np, dtype=torch.float32).to(device)\n    model.eval()\n    with torch.no_grad():\n        logits = model(X_t).cpu().numpy()\n    preds = np.argmax(logits, axis=1)\n    scores = logits[:,1]  # probability-like logit of class 1\n    return preds, scores\n\n\n\n# ======================================================\n# 9. Experiment function (same as SVM version)\n# ======================================================\ndef run_experiment(\n    experiment_name,\n    X,\n    y,\n    train_fraction=0.8,\n    n_components=512,\n    gamma=\"scale\",\n    epochs=60\n):\n    log(f\"==== {experiment_name}: train_fraction={train_fraction:.2f} ====\")\n\n    # Train/eval split\n    X_train, X_eval, y_train, y_eval = train_test_split(\n        X,\n        y,\n        train_size=train_fraction,\n        stratify=y,\n        random_state=42\n    )\n    log(f\"{experiment_name}: Train={X_train.shape}, Eval={X_eval.shape}\")\n\n    # Preprocessing\n    import copy\n    preprocessor = copy.deepcopy(preprocessor_template)\n    preprocessor.fit(X_train)\n\n    X_train_proc = np.array(preprocessor.transform(X_train))\n    X_eval_proc  = np.array(preprocessor.transform(X_eval))\n\n    # Gamma = 'scale'\n    var = X_train_proc.var()\n    n_features = X_train_proc.shape[1]\n    gamma_value = 1.0 / (n_features * var)\n\n    # Nystroem\n    nys = Nystroem(\n        kernel=\"rbf\",\n        gamma=float(gamma_value),\n        n_components=n_components,\n        random_state=42\n    )\n\n    X_train_nys = nys.fit_transform(X_train_proc)\n    X_eval_nys  = nys.transform(X_eval_proc)\n\n    # Train NN\n    model = train_nystroem_nn(\n        X_train_np=X_train_nys,\n        y_train_np=y_train,\n        epochs=epochs,\n        batch_size=512,\n        lr=1e-3,\n        weight_decay=1e-2,\n        patience=50,\n        verbose=True\n    )\n\n    # Eval\n    y_pred, scores = nn_predict(model, X_eval_nys)\n    probs = 1 / (1 + np.exp(-scores))\n\n    f1 = f1_score(y_eval, y_pred, average=\"macro\")\n    auc = roc_auc_score(y_eval, probs)\n    cm = confusion_matrix(y_eval, y_pred)\n\n    print(f\"\\n[{experiment_name}] Eval Macro F1 = {f1:.4f}, AUC={auc:.4f}\")\n    print(f\"[{experiment_name}] Confusion Matrix:\\n{cm}\")\n\n    return {\n        \"experiment\": experiment_name,\n        \"eval_F1\": f1,\n        \"eval_AUC\": auc,\n        \"confusion_matrix\": cm,\n        \"preprocessor\": preprocessor,\n        \"nystroem\": nys,\n        \"model\": model\n    }\n\n\n\n# ======================================================\n# 10. Build mini-dataset (20%)\n# ======================================================\nlog(\"Creating MINI dataset (20%)\")\nX_mini, _, y_mini, _ = train_test_split(\n    X_full, y_full,\n    train_size=0.20,\n    stratify=y_full,\n    random_state=42\n)\n\n\n\n# ======================================================\n# 11. Run experiments\n# ======================================================\nresults_full = run_experiment(\n    experiment_name=\"Full_80_20\",\n    X=X_full,\n    y=y_full,\n    train_fraction=0.80,\n    n_components=512,\n    epochs=60\n)\n\nresults_mini = run_experiment(\n    experiment_name=\"Mini_80_20\",\n    X=X_mini,\n    y=y_mini,\n    train_fraction=0.80,\n    n_components=512,\n    epochs=60\n)\n\n\n\n# ======================================================\n# 12. Comparison\n# ======================================================\nprint(\"\\n======= COMPARISON RESULTS =======\")\nprint(f\"Full_80_20: Macro F1 = {results_full['eval_F1']:.4f}\")\nprint(f\"Mini_80_20: Macro F1 = {results_mini['eval_F1']:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:37:17.609628Z","iopub.execute_input":"2025-12-02T16:37:17.610209Z","iopub.status.idle":"2025-12-02T16:37:47.674247Z","shell.execute_reply.started":"2025-12-02T16:37:17.610185Z","shell.execute_reply":"2025-12-02T16:37:47.673510Z"}},"outputs":[{"name":"stdout","text":"[16:37:17] Using device: cuda | +0.00s\n[16:37:17] Loading datasets... | +0.00s\n[16:37:17] Train shape: (59611, 24), Test shape: (14900, 23) | +0.19s\n[16:37:18] Preprocessing setup ready. | +0.26s\n[16:37:18] Creating MINI dataset (20%) | +0.00s\n[16:37:18] ==== Full_80_20: train_fraction=0.80 ==== | +0.03s\n[16:37:18] Full_80_20: Train=(47688, 23), Eval=(11923, 23) | +0.02s\nEpoch 001 | Loss=0.7223 | Val Macro F1=0.6997\nEpoch 002 | Loss=0.5936 | Val Macro F1=0.7349\nEpoch 003 | Loss=0.5459 | Val Macro F1=0.7436\nEpoch 004 | Loss=0.5305 | Val Macro F1=0.7453\nEpoch 005 | Loss=0.5223 | Val Macro F1=0.7460\nEpoch 006 | Loss=0.5202 | Val Macro F1=0.7457\nEpoch 007 | Loss=0.5188 | Val Macro F1=0.7429\nEpoch 008 | Loss=0.5144 | Val Macro F1=0.7460\nEpoch 009 | Loss=0.5122 | Val Macro F1=0.7454\nEpoch 010 | Loss=0.5116 | Val Macro F1=0.7462\nEpoch 011 | Loss=0.5100 | Val Macro F1=0.7434\nEpoch 012 | Loss=0.5083 | Val Macro F1=0.7489\nEpoch 013 | Loss=0.5082 | Val Macro F1=0.7501\nEpoch 014 | Loss=0.5089 | Val Macro F1=0.7467\nEpoch 015 | Loss=0.5064 | Val Macro F1=0.7457\nEpoch 016 | Loss=0.5047 | Val Macro F1=0.7466\nEpoch 017 | Loss=0.5044 | Val Macro F1=0.7447\nEpoch 018 | Loss=0.5033 | Val Macro F1=0.7436\nEpoch 019 | Loss=0.5016 | Val Macro F1=0.7455\nEpoch 020 | Loss=0.5043 | Val Macro F1=0.7432\nEpoch 021 | Loss=0.5002 | Val Macro F1=0.7474\nEpoch 022 | Loss=0.5021 | Val Macro F1=0.7440\nEpoch 023 | Loss=0.5022 | Val Macro F1=0.7444\nEpoch 024 | Loss=0.5002 | Val Macro F1=0.7452\nEpoch 025 | Loss=0.5013 | Val Macro F1=0.7462\nEpoch 026 | Loss=0.4989 | Val Macro F1=0.7438\nEpoch 027 | Loss=0.4972 | Val Macro F1=0.7445\nEpoch 028 | Loss=0.4977 | Val Macro F1=0.7486\nEpoch 029 | Loss=0.4989 | Val Macro F1=0.7435\nEpoch 030 | Loss=0.4959 | Val Macro F1=0.7459\nEpoch 031 | Loss=0.4958 | Val Macro F1=0.7451\nEpoch 032 | Loss=0.4976 | Val Macro F1=0.7439\nEpoch 033 | Loss=0.4937 | Val Macro F1=0.7468\nEpoch 034 | Loss=0.4958 | Val Macro F1=0.7440\nEpoch 035 | Loss=0.4950 | Val Macro F1=0.7469\nEpoch 036 | Loss=0.4939 | Val Macro F1=0.7437\nEpoch 037 | Loss=0.4931 | Val Macro F1=0.7447\nEpoch 038 | Loss=0.4933 | Val Macro F1=0.7433\nEarly stopping at epoch 38.\nBest validation macro F1: 0.7501\n\n[Full_80_20] Eval Macro F1 = 0.7444, AUC=0.8302\n[Full_80_20] Confusion Matrix:\n[[4157 1513]\n [1528 4725]]\n[16:37:40] ==== Mini_80_20: train_fraction=0.80 ==== | +22.34s\n[16:37:40] Mini_80_20: Train=(9537, 23), Eval=(2385, 23) | +0.01s\nEpoch 001 | Loss=0.7866 | Val Macro F1=0.3079\nEpoch 002 | Loss=0.7400 | Val Macro F1=0.3079\nEpoch 003 | Loss=0.7059 | Val Macro F1=0.3079\nEpoch 004 | Loss=0.6745 | Val Macro F1=0.3979\nEpoch 005 | Loss=0.6425 | Val Macro F1=0.5411\nEpoch 006 | Loss=0.6128 | Val Macro F1=0.5581\nEpoch 007 | Loss=0.5942 | Val Macro F1=0.5637\nEpoch 008 | Loss=0.5829 | Val Macro F1=0.5438\nEpoch 009 | Loss=0.5698 | Val Macro F1=0.5553\nEpoch 010 | Loss=0.5539 | Val Macro F1=0.5755\nEpoch 011 | Loss=0.5464 | Val Macro F1=0.5879\nEpoch 012 | Loss=0.5404 | Val Macro F1=0.6035\nEpoch 013 | Loss=0.5338 | Val Macro F1=0.6009\nEpoch 014 | Loss=0.5332 | Val Macro F1=0.6178\nEpoch 015 | Loss=0.5290 | Val Macro F1=0.6153\nEpoch 016 | Loss=0.5225 | Val Macro F1=0.6280\nEpoch 017 | Loss=0.5225 | Val Macro F1=0.6386\nEpoch 018 | Loss=0.5124 | Val Macro F1=0.6436\nEpoch 019 | Loss=0.5117 | Val Macro F1=0.6432\nEpoch 020 | Loss=0.5088 | Val Macro F1=0.6556\nEpoch 021 | Loss=0.5083 | Val Macro F1=0.6604\nEpoch 022 | Loss=0.5062 | Val Macro F1=0.6549\nEpoch 023 | Loss=0.5004 | Val Macro F1=0.6603\nEpoch 024 | Loss=0.5009 | Val Macro F1=0.6596\nEpoch 025 | Loss=0.5003 | Val Macro F1=0.6582\nEpoch 026 | Loss=0.4966 | Val Macro F1=0.6574\nEpoch 027 | Loss=0.4985 | Val Macro F1=0.6671\nEpoch 028 | Loss=0.4872 | Val Macro F1=0.6529\nEpoch 029 | Loss=0.4886 | Val Macro F1=0.6629\nEpoch 030 | Loss=0.4867 | Val Macro F1=0.6681\nEpoch 031 | Loss=0.4830 | Val Macro F1=0.6602\nEpoch 032 | Loss=0.4786 | Val Macro F1=0.6657\nEpoch 033 | Loss=0.4731 | Val Macro F1=0.6680\nEpoch 034 | Loss=0.4813 | Val Macro F1=0.6577\nEpoch 035 | Loss=0.4760 | Val Macro F1=0.6645\nEpoch 036 | Loss=0.4800 | Val Macro F1=0.6630\nEpoch 037 | Loss=0.4698 | Val Macro F1=0.6617\nEpoch 038 | Loss=0.4692 | Val Macro F1=0.6524\nEpoch 039 | Loss=0.4682 | Val Macro F1=0.6555\nEpoch 040 | Loss=0.4670 | Val Macro F1=0.6501\nEpoch 041 | Loss=0.4565 | Val Macro F1=0.6591\nEpoch 042 | Loss=0.4683 | Val Macro F1=0.6723\nEpoch 043 | Loss=0.4709 | Val Macro F1=0.6556\nEpoch 044 | Loss=0.4548 | Val Macro F1=0.6579\nEpoch 045 | Loss=0.4532 | Val Macro F1=0.6632\nEpoch 046 | Loss=0.4587 | Val Macro F1=0.6553\nEpoch 047 | Loss=0.4550 | Val Macro F1=0.6492\nEpoch 048 | Loss=0.4554 | Val Macro F1=0.6548\nEpoch 049 | Loss=0.4517 | Val Macro F1=0.6453\nEpoch 050 | Loss=0.4467 | Val Macro F1=0.6581\nEpoch 051 | Loss=0.4487 | Val Macro F1=0.6587\nEpoch 052 | Loss=0.4470 | Val Macro F1=0.6598\nEpoch 053 | Loss=0.4495 | Val Macro F1=0.6494\nEpoch 054 | Loss=0.4431 | Val Macro F1=0.6514\nEpoch 055 | Loss=0.4403 | Val Macro F1=0.6520\nEpoch 056 | Loss=0.4363 | Val Macro F1=0.6470\nEpoch 057 | Loss=0.4409 | Val Macro F1=0.6517\nEpoch 058 | Loss=0.4329 | Val Macro F1=0.6520\nEpoch 059 | Loss=0.4471 | Val Macro F1=0.6472\nEpoch 060 | Loss=0.4366 | Val Macro F1=0.6603\nBest validation macro F1: 0.6723\n\n[Mini_80_20] Eval Macro F1 = 0.6616, AUC=0.8214\n[Mini_80_20] Confusion Matrix:\n[[1016  118]\n [ 663  588]]\n\n======= COMPARISON RESULTS =======\nFull_80_20: Macro F1 = 0.7444\nMini_80_20: Macro F1 = 0.6616\n","output_type":"stream"}],"execution_count":13}]}